'''
Created on Aug 22, 2014

@author: TPB
'''

import csv
from math import floor
from os.path import exists
from threading import Lock
import time

#from pyutils
from PyUtils.DebugLog import TBLogger
from PyUtils.Files import filesinpath,string_filter
from PyUtils.ezTime import ezTime, ts2epoch 
from PyUtils.CircularBuffer import data_buffer
import GlobalConfig

#logging
CSVroot = TBLogger(GlobalConfig.logpath+__name__+'.out','warning','CSV',streamLevel=None)

class csvHandler(object):
    ''' csvHandler: A wrapper method for handling reading and writing operation to csv files
    '''
    #set lock object for synchronizing thread access
    lock = Lock() 
    
    def __init__(self, keys = list(), mode ='wt',outputfile = None, inputfile = None, buffers = True,overWriteExisting=False):
        '''
        @param keys list of keys (excluding Time) indicating Sensor values to be logged
        @param mode flags    for csv write operations a = append w = write over
        @param outputfile    string to be used to label output .csv
        @param inputfile     if required, separate file for reading in CSV data       '''
        self.logger = CSVroot.MakeChild('Handler')
        self.logger.debug('CSV Handler Initialized with keys %s' %keys)
        self.ext = '.csv'
        self.buffers = buffers
        self.keys = ['Time', 'Epoch Time'] + keys
        self.dbkeys = keys + ['Epoch Time']
        self.eztime = ezTime()
        self.mode = mode
        self.inpath = inputfile
        self.badrows = []
        self.outpath = outputfile
        self.logperiod = 1
        #Setup Pickled Lists
        
        if self.buffers == True:
            self.initBuffers()
            
        if outputfile:
            self.rootname = outputfile.split(self.ext)[0]
            if self.mode == 'a' or self.mode == 'rt':    #use the same old file in append mode
                self.outpath = self.rootname + self.ext     #full name of autogenerated csv file
            elif overWriteExisting and 'w' in self.mode:
                self.outpath = self.rootname + self.ext     #full name of autogenerated csv file
            else:
                self.setFileName()  #make a new rotating file handler
            if not exists(self.outpath):    #write a header row only if file does not yet exist
                self.setHeader()
            if inputfile == outputfile:
                self.inpath = self.outpath #sets input file path to result of automatic name generator's log file 

    def initBuffers(self):
        self.p_ext = '.pickle'
        self.writecounter = 0
        self.max_count = 120
        self.buffkeys = ['Hour', '4 Hour', 'Day', 'Week']
        self.buffkeys2time = {'All':(3600*24*90), #up to 90 days in the past - so not really all, but should be pretty close
                        'Month':(3600*24*30), #30 days previous
                        'Week':(3600*24*7), #etc
                        'Day':(3600*24),
                        '4 Hour':(3600*4),
                        'Hour':3600}
        self.picklepaths = {'Hour':GlobalConfig.logpath+"hour.pickle",'4 Hour': GlobalConfig.logpath+"4hour.pickle" , 'Day': GlobalConfig.logpath+"day.pickle", 'Week': GlobalConfig.logpath+"week.pickle"} 
        self.initializeBuffers()
        self.checkOldData()
        
    def initializeBuffers(self):
        self.data_buffers = dict.fromkeys(self.buffkeys)
        for key in self.data_buffers:
            if key == 'Hour': self.data_buffers[key] = data_buffer(self.dbkeys,168,GlobalConfig.logpath,key)
            self.data_buffers[key] = data_buffer(self.dbkeys,self.max_count,GlobalConfig.logpath,key)
    
    def checkOldData(self):
        file_list = filesinpath([GlobalConfig.logpath])
        pickles = string_filter(file_list,['.pickle'])
        print 'files found on disk:', pickles
        for key in self.buffkeys:
            if (key+self.p_ext) in pickles: 
                print '%s found on file system: loading data into buffer' %key
                self.data_buffers[key].loadBuffer(store = True)
        #return pickles
    
    def setFileName(self):
        ''' Rotating File Naming method: run by self.init if file method is not set to append
        naming convention = outputfile + current date, adds .i to end if file exists until a unused name is found'''
        datestamp = self.eztime.getDate('_')
        newfilename = self.rootname + '_' + datestamp
        outputfile = newfilename + self.ext
        i = 1
        while exists(outputfile):
            self.logger.error('File %s already Exists' %outputfile)
            outputfile = newfilename + '.%s' %i + self.ext
            i = i + 1
            self.logger.error('Attempting to Rename File as : %s' %outputfile)
        self.outpath = outputfile
        self.logger.info('Unique CSV successfully created as : %s' %outputfile)
        self.setHeader() 
        
    def setHeader(self):
        ''' Writes first line to the csv file with contents of keys'''
        with open(self.outpath, self.mode) as f:
            writer = csv.DictWriter(f, self.keys)
            writer.writeheader()
            
    def writeLine(self, data):
        ''' writes a single line to the csv file from a dictionary of keys
        '''
        with open(self.outpath, 'a') as f:
            writer = csv.DictWriter(f, self.keys)
            writer.writerow(data)
            #writer.writerow( str(data['Time']), str(data['DO']), str(data['pH']), str(data['Freq']), str(data['Temp']) )
    
    def readLines(self,startn = 0,stopn = -1):
        '''reads a segment of the csvfile from the index's startn:stopn'''
        with open(self.inpath,'rt') as f:
            reader = csv.reader(f)
            lines = list(reader)[startn:stopn]
        return lines
        
    def readFirstLine(self):
        ''' reads the first line of the csv file - this should be a header line of column lables'''
        self.logger.debug('readFirstLine called')
        with open(self.inpath,'rt') as f:
            reader = csv.reader(f)
            header = list(reader)[0]
        return header
        
    def getDataList(self, datadict):
        '''reads the entire csv file into memory and returns it as a list.
        datalist is a set of 3 nested lists as datalist[C][T][I]
        [C] = csv column [0 - len(keys)]    top order list, contains the other two nested lists
        [T] = type [0 = key, 1 = data values (list)]    middle order list, a single string name
        [I] = [data value list index]    lowest order list of floats (i.e. the raw data)'''
        datalist = datadict.items()
        return datalist
    
    def getDataListKeys(self, datalist):
        '''gets a list of dictionary keys from the header line (first row) of a CSV file'''
        keys = []
        for row in datalist:
            keys.append(row[0])
        return keys
        
    def readLastLine(self, filen = False):
        if filen:
            filename = filen
        else:
            filename = self.inpath
        with open(filename, 'rt') as f:
            reader = csv.reader(f)
            lastline = list(reader)[-1]
        return lastline
   
    def readDataWithRowHeaders(self): 
        with open(self.inpath, 'rt') as f:
            dreader = csv.reader(f)
            rowNum = 0
            dataDict = dict()
            for row in dreader:
                if rowNum == 0:
                    header = row
                else:
                    rowData = row
                    key = row.pop(0)
                    dataDict[key]=row
                rowNum+=1
        return dataDict

                
    def readRawDict(self):
        with open(self.inpath, 'rt') as f:
            dreader = csv.DictReader(f)
            mylist = []
            for row in dreader:
#                 pp(row)
                for key,value in row.iteritems():
                    if value == None or value == '': value = 0
                    row[key] = value
                mylist.append(row)
        self.logger.debug('Full Data Dict Read In')
        return mylist
    
    def readRawDictInt(self):
        with open(self.inpath, 'rt') as f:
            dreader = csv.DictReader(f)
            mylist = []
            for row in dreader:
                for key,value in row.iteritems():
                    if value == None or value == '': value = 0
                    row[key] = int(value)
                mylist.append(row)

        self.logger.debug('Full Data Dict Read In')
        return mylist
    
    def readDataDict(self, keys = None):
        '''Method to read in entire CSV file as a dictionary.
        Should only be called once, returns a dictionary with empty rows removed'''
        if not keys:
            keys = self.keys
        tdict = dict.fromkeys(keys)
        self.logger.debug('readDataDict called')
        with open(self.inpath, 'rt') as f:
            dreader = csv.DictReader(f)            
            self.logger.debug('Dict Reader Instantiated')
            for i in range(len(keys)):
                tdict[keys[i]] = []
            self.logger.debug('Empty Dictionary Created')
            for row in dreader:
                self.logger.debug('Row # %s Read In' %row)
                for i in range(len(keys)):
                    sdat = row[keys[i]]
                    if keys[i] == 'Epoch Time':
                        sdat = float(sdat)
                    tdict[keys[i]].append(sdat)
        #cleandict = self.cleanDataDict(tdict)
        self.logger.debug('Full Data Dict Read In')
        return tdict
    
    def cleanDataDict(self, data):
        '''removes anything that has an improper timestamp from the dataset
        @type data:    dictionary''' 
        self.logger.debug('Clean Data Dict Called')
        for i in range(len(self.keys)):
            data[self.keys[i]] = self.clearbadrows(data[self.keys[i]])
        return data

    def parseDataSegmentIndex(self,epochtime, elapsedtimes, starttime, endtime, points = 10):
        '''Given a start and end time (epoch time format), returns the nearest values in the cumulative time delta list
        returns [start time value, end time value]. 
        Solves for datasegment by the method of bisections.'''
        self.logger.debug('parseDataSegmentIndex Called')
        gtsLog = CSVroot.MakeChild('getTS')
        end_found = False
        start_found = False
        et = elapsedtimes
        start_list = et
        end_list = et
        ept = epochtime
        dt_range = endtime - starttime
        timeofcall = time.time()
        time_offset = timeofcall - ept[-1] #returns time since last sensor reading
        time_delta_zero = ept[-1] # abs time of last sensor reading (delta = 0) , i = 0)
        if starttime <= ept[0]:
            gtsLog.warning('Start Time Requested preceededs dataset, all data returned')
            starttime = ept[0] #return oldest available data
            start_delta = et[-1]
            start_val = start_list[-1]
            start_found = True
        elif starttime > ept[-1]:
            gtsLog.error('Error: Invalid Start Time: no data exists wtihin these bounds')
        else:
            start_delta = time_delta_zero - starttime # seconds since first point to plot :: Oldest point to plot
        if endtime >= ept[-1]:
            endtime = ept[-1]
            end_delta = et[0]
            end_val = end_list[0]
            end_found = True
            gtsLog.warning('End Time Requested exceeds dataset, all data returned')
        elif endtime < ept[0]:
            gtsLog.error('Error: Invalid End Time: no data exists within these bounds')
        else:
            end_delta =  time_delta_zero - endtime # time since last point to plot Most Recent Point to plot
        gtsLog.info('List Start: %s    List End: %s    Delta: %s' %(ept[0], ept[-1], ept[-1]-ept[0]))
        gtsLog.info('Start Delta: %s    End Delta : %s' %(start_delta, end_delta))
        list_size = len(et)
        gtsLog.info('Data List Size: %s' %list_size)
        si_end = len(et)
        ei_end = len(et)
        gtsLog.info('Start Found: %s    End Found: %s' %(start_found, end_found))
        while start_found == False:
            sl_size = len(start_list)
            gtsLog.debug('Start List Size: %s' %sl_size)
            gtsLog.debug('Half Size: %s' %(sl_size/2))
            if sl_size == 1:
                ei_end = si_end #shortens end_list to necessary data
                start_found = True
                start_val = start_list[0]
                gtsLog.debug('Starting Delta Found: %s' %start_list)
                break
            if start_list[sl_size/2] > start_delta:
                start_list = start_list[:sl_size/2]
                #si_end = sl_size/2 #remove last half of list
                gtsLog.debug('Second Half Cut')
            elif start_list[sl_size/2] < start_delta:
                start_list = start_list[sl_size/2:]
                #si_beg = sl_size/2
                gtsLog.debug('First Half Cut')
            elif start_list[sl_size/2] == start_delta:
                temp_val = start_list[sl_size/2]
                start_list = list()
                start_list.append(temp_val)
                gtsLog.debug('Start Delta Found at halfway mark, list remade to : %s' %start_list)
        while end_found == False:
            el_size = len(end_list)
            gtsLog.debug('End List Size: %s' %el_size)
            gtsLog.debug('Half Size: %s' %(el_size/2))
            if el_size == 1:
                end_found = True
                gtsLog.debug('Ending Delta Found: %s' %end_list)
                end_val = end_list[0]
                break
            if end_list[el_size/2] > end_delta:
                end_list = end_list[:el_size/2]
                #si_end = el_size/2 #remove last half of list
                gtsLog.debug('Second Half Cut')
            elif end_list[el_size/2] < end_delta:
                end_list = end_list[el_size/2:]
                #si_beg = el_size/2
                gtsLog.debug('First Half Cut')
            elif end_list[el_size/2] == end_delta:
                temp_val = end_list[el_size/2]
                end_list = list()
                end_list.append(temp_val)
                gtsLog.debug('end Delta Found at halfway mark, list remade to : %s' %end_list)
        gtsLog.info('Elapsed Time Since oldest data point requested found: %s' %start_val)
        gtsLog.info('Elapsed Time Since most recent data point requested found: %s' %end_val)
        datai = [start_val, end_val]
        return datai 
            
    def getDataDicts(self,starttime, endtime , returnkeys = None, points = 100):
        '''
        ***called directly from ctlsys through QMLInterface
        Method to return a segment of the csv data from start time to end time
        calls function to read in entire csv file
        times must be in epoch time
        returnkeys must be list of strings that match dictionary keys and csv header lines
        
        @param starttime:    time of the OLDEST data point requested
        @type starttime:    float seconds since the epoch
        @param endtime:    time of the NEWEST data point requested
        @type endtime:    float second since the epoch
        @param returnkeys:    names for CSV data headers (must match EXACTLY!!)
        @type returnskeys:    list of strings
        @param points:    number of datapoints to return
        @type points:    int'''

        gdsLog = CSVroot.MakeChild('getData')
        self.logger.debug('getData Segment Called')
        gdsLog.info('Keys requested from getDataDicts: %s' %returnkeys)
        self.lock.acquire()
        gdsLog.debug('Lock Acquired by getData')
        self.eztime.reset()
        
        if not returnkeys:  #Returns all data vectors unless specified otherwise
            returnkeys = self.keys

        returnkeys = returnkeys + ['Epoch Time']
        fulldatadict = self.readDataDict(); gdsLog.info('Full CSV File read into memory')

        if len(fulldatadict['Time']) < 5:
            gdsLog.warning('Data Log of %s points is not enough data to plot' %len(fulldatadict['Time']))
            self.lock.release()
            return(dict.fromkeys(returnkeys))
        
        gdsLog.debug('Available Keys from CSV: %s' %fulldatadict.keys())
        #epoch_time = self.parseEpochTime(fulldatadict); gdsLog.debug('Time Since Epoch Parse Complete')
        epoch_time = fulldatadict['Epoch Time']
        cleandatadict = self.cleanDataDict(fulldatadict)
        del fulldatadict #remove dict data from memory
        #calculate some time info
        delta_time = self.calcTimeDeltas(epoch_time); gdsLog.debug('Delta times Parse Complete')
        elapsed_time = self.calcElapsedTime(delta_time); gdsLog.debug('Cumulative Delta parse complete')
        cleandatadict['Epoch Time'] = epoch_time
        gdsLog.debug('Available Keys from cleandatadict %s' %cleandatadict.keys())

        fulldatalist = self.getDataList(cleandatadict)
        listkeys = self.getDataListKeys(fulldatalist)
        
        datadict=dict().fromkeys(returnkeys)

        gdsLog.debug('Available Keys from CleanDataList %s' %listkeys)
        #debug logging
        gdsLog.debug('CSV Data converted to list')        
        gdsLog.debug('Data is Available in range from %s : %s' %(epoch_time[0], epoch_time[-1]))
        gdsLog.debug('Duration of Available Data: %s' %(epoch_time[-1] - epoch_time[0]))
        gdsLog.debug('Start time (%s) must be before end of file (%s)' %(starttime, epoch_time[-1]))
        gdsLog.debug('End time (%s) must be after start of file (%s)' %(endtime, epoch_time[0]))
        gdsLog.debug('Starttime %s::Endtime %s :: Difference %s' %(starttime, endtime, starttime-endtime))
        gdsLog.debug('Starttime requested - end of file must be > 0, it is %s' %(starttime - epoch_time[-1]))
        gdsLog.debug('Endtime requested - start of file must be > 0, it is %s' %(endtime - epoch_time[0]))
        #sterilize input data requests
        if starttime < epoch_time[0]:
            gdsLog.info('Start Time Requested preceededs dataset, all data returned')
            #starttime = epoch_time[0] #return oldest available data'''
            pass
        elif starttime > epoch_time[-1]:
            gdsLog.error('Error: Invalid Start Time: no data exists wtihin these bounds')
            return datadict
        
        if endtime > epoch_time[-1]:
            # endtime = epoch_time[-1]
            gdsLog.info('End Time Requested exceeds dataset, all data returned')
            pass
        elif endtime < epoch_time[0]:
            gdsLog.error('Error: Invalid End Time: no data exists within these bounds')
            return datadict
        segment = self.parseDataSegmentIndex(epoch_time,elapsed_time, starttime, endtime)
        
        gdsLog.info('Segment Returned from getTS as %s' %(segment))
        self.logger.debug('List Keys Returned as %s, return keys requested : %s' %(listkeys, returnkeys))
        
        keysindex= []
        for i in range(len(listkeys)):
            keysindex.append(listkeys.index(str(listkeys[i])))
            gdsLog.debug('Key index for: %s found at %s' %(listkeys[i], keysindex[i]))
        
        starti = elapsed_time.index(segment[0])
        endi = elapsed_time.index(segment[1])
        gdsLog.info('Index For requested Data Set: Start: %s Stop: %s' %(starti, endi))
        etlen = len(elapsed_time)
        ddlen = len(fulldatalist[0][1])
        fulllength = -endi - -starti
        gdsLog.info('List Lengths; ElapsedTimes: %s Datadict: %s Segment: %s' %(etlen, ddlen, fulllength))
        for i in range(len(returnkeys)):
            #NOTE: datadict values are strings from the CSV reader
            if endi == 0:
                datadict[str(returnkeys[i])] = fulldatalist[listkeys.index(str(returnkeys[i]))][1][-starti:]
            else:
                datadict[str(returnkeys[i])] = fulldatalist[listkeys.index(str(returnkeys[i]))][1][-starti:-endi]
        
        gdsLog.info('CSV Reading Request completed in %s seconds' %self.eztime.getDelta())
        parseddict = self.parseDataSegment(datadict, points, average = True)
        self.lock.release()
                
        return parseddict   #returns a dictionary of {<variable name>:<list of floating point data>}
    
    def parseDataSegment(self, datadict, points = 100, average = True, raw = False):
        '''Takes an arbitrary data dictionary of {<variable name>:<list of string numbers read from CSV>} and returns a floating point dictionary, 
        moving averaged to the specified number of points''' 
        self.logger.debug('parseDataSegment called for %s' %datadict.keys())
        rawlen = len(datadict[str(datadict.keys()[0])]) 
        self.logger.debug('The %s pt dictionary will be reduced to %s points' %(rawlen, points))
        splice_val = floor(rawlen/points)
        self.logger.debug('Each Returned point will be the average of the previous %s points' %splice_val)
        #convert to floating point types for return to mathy parts of the control system
        for name in datadict.keys():
            datadict[name] = list(float(i) for i in datadict[name][:])
        #save us some computation of the requested dataset is already longer than the number of requested points
        if splice_val  < 1:
            return datadict
        parseddict=dict()
        for key, datalist in datadict.items():
            parsedlist = list()
            for n in range(points):
                splice_start = int(n*splice_val)
                ##print splice_start
                splice_end = int((n+1) * splice_val)
                ##print splice_end
                currlist = datalist[splice_start:splice_end] #copy on assignment or a pointer to a subset of the data? profile this part
                if average == True: #Apply scaled moving average to data to return requested number of points
                    curr_avg = sum(iter(currlist))/splice_val
                    parsedlist.append(curr_avg)
                elif average == False: #Return raw values split out of the list, i.e. return every 5th datum
                    curr_val = datalist[splice_start]
                    parsedlist.append(curr_val)
            parseddict[str(key)] = parsedlist
        return parseddict
                
    def parseEpochTime(self, datadict = None):
        ''' Generates self.timelog which is a list of time data formated as:
        [raw epoch time, time delta between each reading, cumulative delta time]
        '''
        if not datadict:
            datadict = self.readDataDict()
        time = datadict['Time']
        et=[]
        for i in range(len(time)):
            try:
                et.append(ts2epoch(time[i]))
            except ValueError:
                self.logger.error('Row # %s Not Formatted Correctly, removing row' %i)
                et.append(0000)
                self.badrows.append(i)
        etc = self.clearbadrows(et)
        return etc
        
    def calcTimeDeltas(self, et):
        '''calculates the delta between each line in the csv timestamp
        from the raw epoch times'''
        dt=[]
        for row in range(len(et))[1:]:
            dt.append(float(et[row]) - float(et[row - 1]))
        return dt
    
    def calcElapsedTime(self, dt):
        '''Method calculates elapsed time of timeseries data'''
        dt_reversed = dt
        dt_reversed.reverse()
        elapsed_time = [0]
        for i in range(len(dt)):
            elapsed_time.append(elapsed_time[i] + dt_reversed[i])
        return elapsed_time
        
    def clearbadrows(self, data):
        ''' removes badrows from the list "data"'''
        for i in range(len(self.badrows)):
            row = self.badrows[i]
            crap = data.pop(row - i)
            self.logger.info('Bad Data " %s " Removed from row # %s' %(crap, row))
        return data
    
    def recordData(self,data):
        # Write Line to csv
        self.writeLine(data)
        if self.buffers: self.recordDataBuffers(data)
        
    def recordDataBuffers(self,data):
        ''' writes a single line to the csv file from a dictionary of keys
        '''
        #data_paths = [(out_hour, self.data_hour),(out_4hour,self.data_4hour),(out_day,self.data_day),(out_week,self.data_week)]

        #write to data buffer
        self.data_buffers['Hour'].Write(data)
        #print 'hour buffer:', self.data_buffers['Hour'].buff
        if self.data_buffers['Hour'].isFull(): self.data_buffers['Hour'].saveBuffer()
        self.writecounter +=1 
        if self.writecounter%4 == 0: #4th write operation        
            self.data_buffers['4 Hour'].Write(self.data_buffers['Hour'].ReadAvg(4))
            if self.data_buffers['4 Hour'].isFull(): self.data_buffers['4 Hour'].saveBuffer() 
            if self.writecounter%24 == 0:
                self.data_buffers['Day'].Write(self.data_buffers['Hour'].ReadAvg(24))
                if self.data_buffers['Day'].isFull(): self.data_buffers['Day'].saveBuffer() 
                if self.writecounter%168 == 0:
                    self.data_buffers['Week'].Write(self.data_buffers['Hour'].ReadAvg(168))
                    self.data_buffers['4 Hour'].saveBuffer()
                    self.data_buffers['Day'].saveBuffer()
                    self.data_buffers['Week'].saveBuffer()
    
    
    def getGraphData(self,keys = None , timerange = 'Hour'):
        if timerange in self.buffkeys:
            data2return = self.data_buffers[timerange].returnKeys(keys)
        elif timerange == 'All' or 'Month':
            data2return = self.data_buffers['Week'].returnKeys(keys)
        return data2return
        


        




